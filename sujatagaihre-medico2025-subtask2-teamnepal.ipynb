{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -U medvqa","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install -U medvqa\n!medvqa validate_and_submit --competition=medico-2025 --task=1 --repo_id=SujataGaihre/Kvasir-VQA-x1-lora_250915-2004","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip cache purge\n!conda clean -a -y\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nfrom peft import PeftModel\n\n# base model (foundation model you trained on)\nBASE_MODEL = \"google/paligemma-3b-pt-224\"\n# your LoRA repo (fine-tuned weights you pushed to HF)\nLORA_MODEL = \"SujataGaihre/Kvasir-VQA-x1-lora_250918-2030\"\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# load processor from the base model\nprocessor = AutoProcessor.from_pretrained(BASE_MODEL)\n\n# load base model\nmodel = AutoModelForVision2Seq.from_pretrained(BASE_MODEL, torch_dtype=torch.float16, device_map=\"auto\")\n\n# apply LoRA adapter\nmodel = PeftModel.from_pretrained(model, LORA_MODEL)\nmodel.to(device)\n\nprint(\"âœ… Base + LoRA model loaded successfully\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import openai\n\nopenai.api_key = \"ADD_API_KEY\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# --- 3. Helper Function ---\ndef ask_model(image, question, model, processor, device):\n    \"\"\"\n    Ask the VQA model a question and return the generated text answer.\n    \"\"\"\n    inputs = processor(images=image, text=question, return_tensors=\"pt\").to(device)\n    generated_ids = model.generate(**inputs, max_new_tokens=50)\n    answer = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n    return answer\n\n# --- 4. Main Pipeline ---\ndef main():\n    print(\"ðŸš€ Starting solution for Medico-2025 Subtask 2: Multimodal Explanations\")\n\n    # Load official validation set\n    print(\"Loading and preparing the official val_set_task2...\")\n    ds = load_dataset(\"SimulaMet/Kvasir-VQA-x1\")[\"test\"]\n    val_set_task2 = (\n        ds.filter(lambda x: x[\"complexity\"] == 1)\n          .shuffle(seed=42)\n          .select(range(1500))\n          .add_column(\"val_id\", list(range(1500)))\n          .remove_columns([\"complexity\", \"answer\", \"original\", \"question_class\"])\n          .cast_column(\"image\", HfImage())\n    )\n    print(f\"âœ… Official validation set loaded with {len(val_set_task2)} samples.\")\n\n    print(f\"Generating explanations and writing to {OUTPUT_FILE}...\")\n    VISUALS_DIR.mkdir(exist_ok=True)\n\n    with open(OUTPUT_FILE, \"w\") as f:\n        for item in tqdm(val_set_task2, desc=\"Generating explanations\"):\n            val_id = item[\"val_id\"]\n            img_id = item[\"img_id\"]\n            image = item[\"image\"]\n            original_question = item[\"question\"]\n\n            # Step 1: Main answer\n            main_answer = ask_model(image, original_question, model, processor, device)\n\n            # Step 2: Probing questions\n            probe_questions = [\n                \"What is the primary finding or subject in this image?\",\n                \"Where is the finding located in the image (e.g., upper-left, center)?\",\n                \"Describe the visual characteristics of the finding, such as color, shape, and texture.\"\n            ]\n            probe_answers = [ask_model(image, q, model, processor, device) for q in probe_questions]\n\n            # Step 3: Textual explanation synthesis\n            textual_explanation = (\n                f\"The model's answer '{main_answer}' is justified by the following observations. \"\n                f\"The primary finding identified is '{probe_answers[0]}'. \"\n                f\"This is located in the {probe_answers[1]} region of the endoscopic view. \"\n                f\"Visually, the finding is characterized by '{probe_answers[2]}'. \"\n                \"This combination of features supports the conclusion.\"\n            )\n\n            # Step 4: Format output\n            result = {\n                \"val_id\": val_id,\n                \"img_id\": img_id,\n                \"question\": original_question,\n                \"answer\": main_answer,\n                \"textual_explanation\": textual_explanation,\n                \"visual_explanation\": [],   # placeholder for heatmaps/bboxes\n                \"confidence_score\": 0.95    # placeholder score\n            }\n\n            f.write(json.dumps(result) + \"\\n\")\n\n    print(f\"\\n Success! Your submission file '{OUTPUT_FILE}' has been created.\")\n    print(\"This file is ready to upload to your Hugging Face submission repo.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mapping = {\n  \"abnormality_presence\": [\"Where in the image is the abnormality located?\", \"What color is the abnormality?\", \"What type of abnormality is present?\", \"How many abnormalities are visible?\", \"Is the abnormality easy to detect?\", \"What is the size of the abnormality?\", \"Does the abnormality involve a polyp?\", \"Is there text overlapping the abnormality?\", \"Is there an instrument near the abnormality?\", \"Is the abnormality clinically significant?\"],\n  \"landmark_presence\": [\"Where is the landmark located in the image?\", \"What color is the landmark?\", \"Is the landmark easy to detect?\", \"Is the landmark partially obscured by an instrument?\", \"Is there a finding near the landmark?\", \"Is text overlapping the landmark?\", \"How many landmarks are visible?\", \"Is this landmark specific to colonoscopy or gastroscopy?\", \"Does the landmark confirm the procedure type?\", \"Is this landmark used as a reference for polyp detection?\"],\n  \"instrument_presence\": [\"Where in the image is the instrument located?\", \"How many instruments are present?\", \"What type of instrument is visible?\", \"Is the instrument interacting with a polyp?\", \"Is the instrument interacting with a landmark?\", \"Is the instrument obscuring any abnormality?\", \"Is the instrument easy to detect?\", \"Does the instrument suggest ongoing treatment or biopsy?\", \"What is the orientation of the instrument (left, right, center)?\", \"Is the instrument metallic or plastic in appearance?\"],\n  \"polyp_removal_status\": [\"How many polyps are present?\", \"Where are the remaining polyps located?\", \"What type of polyp is visible?\", \"What is the size of the remaining polyp?\", \"Is an instrument visible near the polyp?\", \"Does the mucosa appear healed after removal?\", \"Is there evidence of a resection site?\", \"Is bleeding visible after removal?\", \"Is clipping or cauterization present?\", \"Is the removal complete or partial?\"],\n  \"finding_detectability\": [\"What is the color of the finding?\", \"Where in the image is the finding?\", \"How many findings are present?\", \"Is the finding small or large?\", \"Is the finding obscured by an instrument?\", \"Is the finding obscured by text or artifacts?\", \"What is the type of finding (polyp, lesion, inflammation)?\", \"Is the finding clinically relevant?\", \"Is the finding localized or diffuse?\", \"Does the finding resemble common abnormalities?\"],\n  \"box_artifact_presence\": [\"Where in the image is the artifact located?\", \"What color is the artifact (green or black)?\", \"Is the artifact covering an abnormality?\", \"Is the artifact covering an instrument?\", \"Is the artifact covering a landmark?\", \"Is the artifact small or large?\", \"Does the artifact reduce visibility of findings?\", \"Is the artifact near the image borders?\", \"Is there text overlapping the artifact?\", \"Is the artifact affecting detectability of polyps?\"],\n  \"text_presence\": [\"Where in the image is the text located?\", \"Is the text overlapping a finding?\", \"Is the text overlapping a landmark?\", \"Is the text overlapping an instrument?\", \"Is the text easy to read?\", \"Is the text color white or another color?\", \"Is the text obstructing detectability?\", \"Is the text medical annotation or timestamp?\", \"Is the text large or small?\", \"Is there more than one text element?\"],\n  \"polyp_type\": [\"Where is the polyp located in the image?\", \"What is the size of the polyp?\", \"What color is the polyp?\", \"How many polyps are visible?\", \"Is the polyp easy to detect?\", \"Is the polyp sessile or pedunculated?\", \"Is there an instrument near the polyp?\", \"Is the polyp associated with bleeding?\", \"Is the polyp removal complete?\", \"Is the polyp adjacent to a landmark?\"],\n  \"procedure_type\": [\"Is a landmark confirming the procedure visible?\", \"Are polyps present typical for this procedure type?\", \"Is the mucosa appearance consistent with the procedure?\", \"Is there an instrument suggesting this procedure type?\", \"Is capsule endoscopy text visible?\", \"Does the lumen size suggest colonoscopy or gastroscopy?\", \"Is this a lower or upper GI procedure?\", \"Are abnormality types typical for this procedure?\", \"Is light reflection consistent with endoscopic imaging?\", \"Is the orientation of the image typical for this procedure?\"],\n  \"polyp_size\": [\"Where is the polyp located in the image?\", \"What type of polyp is it?\", \"What color is the polyp?\", \"Is the polyp sessile or pedunculated?\", \"Is the polyp adjacent to a landmark?\", \"Is there more than one polyp?\", \"Is the polyp removal complete?\", \"Is an instrument present near the polyp?\", \"Is the polyp easy to detect?\", \"Is the polyp size clinically significant?\"],\n  \"finding_count\": [\"How many polyps are present?\", \"How many abnormalities are present?\", \"How many landmarks are visible?\", \"How many instruments are visible?\", \"How many findings are clinically relevant?\", \"How many findings are small vs large?\", \"How many findings are overlapping?\", \"How many findings are obscured by artifacts?\", \"How many findings are located in the center?\", \"How many findings are located near borders?\"],\n  \"polyp_count\": [\"Where are the polyps located?\", \"What is the size of each polyp?\", \"What is the type of each polyp?\", \"What color are the polyps?\", \"Are polyps clustered together?\", \"Are polyps easy to detect?\", \"Are any polyps overlapping with landmarks?\", \"Are polyps interacting with instruments?\", \"Is polyp removal complete?\", \"Are polyps causing mucosal distortion?\"],\n  \"instrument_location\": [\"What type of instrument is at this location?\", \"Is the instrument near an abnormality?\", \"Is the instrument near a polyp?\", \"Is the instrument near a landmark?\", \"Is the instrument in the center of the lumen?\", \"Is the instrument touching tissue?\", \"Is the instrument obscuring findings?\", \"Is the instrument easy to detect?\", \"Is there more than one instrument?\", \"Is the instrument metallic or plastic?\"],\n  \"abnormality_location\": [\"What type of abnormality is at this location?\", \"What is the size of the abnormality?\", \"What is the color of the abnormality?\", \"Is the abnormality easy to detect?\", \"Is the abnormality overlapping with text?\", \"Is the abnormality overlapping with instruments?\", \"Is the abnormality adjacent to a landmark?\", \"Is the abnormality in the lumen center?\", \"Are there multiple abnormalities nearby?\", \"Is the abnormality clinically significant?\"],\n  \"landmark_location\": [\"What type of landmark is at this location?\", \"What is the color of the landmark?\", \"Is the landmark easy to detect?\", \"Is the landmark partially obscured?\", \"Is the landmark overlapping with text?\", \"Is the landmark adjacent to an abnormality?\", \"Is the landmark in the lumen center?\", \"Are there multiple landmarks visible?\", \"Is the landmark used for orientation?\", \"Is the landmark confirming the procedure type?\"],\n  \"instrument_count\": [\"What type of instruments are visible?\", \"Where are the instruments located?\", \"Are the instruments interacting with polyps?\", \"Are the instruments interacting with abnormalities?\", \"Are the instruments obscuring landmarks?\", \"Are the instruments metallic or plastic?\", \"Are the instruments easy to detect?\", \"Are any instruments overlapping?\", \"Do the instruments suggest biopsy or resection?\", \"Is the number of instruments typical for this procedure?\"],\n  \"abnormality_color\": [\"Where is the abnormality located?\", \"What type of abnormality is this?\", \"What is the size of the abnormality?\", \"Is the abnormality easy to detect?\", \"Are there multiple colors within the abnormality?\", \"Is the color typical of inflammation or bleeding?\", \"Does the abnormality color suggest necrosis?\", \"Is the abnormality overlapping with text?\", \"Is the abnormality near an instrument?\", \"Is the abnormality color clinically significant?\"],\n  \"landmark_color\": [\"Where is the landmark located?\", \"What type of landmark is this?\", \"Is the landmark easy to detect?\", \"Are there multiple colors in the landmark?\", \"Is the landmark partially obscured by instruments?\", \"Is the landmark partially obscured by text?\", \"Is the landmark used for orientation?\", \"Is the landmark color typical of healthy mucosa?\", \"Does the landmark color suggest inflammation?\", \"Is the landmark color clinically relevant?\"],\n  \"finding_presence\": [\"Where is the finding located?\", \"What type of finding is this?\", \"What is the size of the finding?\", \"What is the color of the finding?\", \"Is the finding easy to detect?\", \"How many findings are present?\", \"Is the finding overlapping with text?\", \"Is the finding overlapping with instruments?\", \"Is the finding clinically significant?\", \"Does the finding suggest further biopsy?\"]\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nfrom peft import PeftModel\nfrom datasets import load_dataset, Dataset, Image as HfImage\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport json\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"ADD_API_KEY\")\n\n# --- File paths ---\nOUTPUT_FILE = \"FINAL_prompt_OPENAI_submission_task2.jsonl\"\nVISUALS_DIR = Path(\"visuals\")\nVISUALS_DIR.mkdir(exist_ok=True)\n\n# --- Helper: remove echoed question from answer ---\ndef clean_answer(question: str, answer: str) -> str:\n    q_lower = question.lower().strip(\" ?\")\n    a_lower = answer.lower().strip()\n\n    if a_lower.startswith(q_lower):\n        # strip the question part from the answer\n        return answer[len(question):].strip(\" ,.?\")\n    return answer.strip()\n\n# --- Helper: ask model ---\ndef ask_model(image, question):\n    \"\"\"Get answer from the vision-language model.\"\"\"\n    inputs = processor(images=image, text=question, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        generated_ids = model.generate(**inputs, max_new_tokens=50)\n    answer = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n    return answer\n\n# --- Helper: summarize with OpenAI ---\ndef summarize_with_openai(main_answer, mapped_answers):\n    prompt = f\"\"\"\n    You are a medical AI assistant. Your task is to generate a clear and concise textual explanation \n    based on a main answer and several follow-up question-answer pairs. \n\n    Main Answer: {main_answer}\n    Follow-up Observations: {mapped_answers}\n\n    Instructions:\n    - Combine all information into a single coherent explanation in natural language. \n    - Avoid repeating the exact questions. Instead, integrate the answers into flowing sentences.\n    - Present findings logically (location â†’ type â†’ size â†’ detectability â†’ color â†’ clinical significance).\n    - If there are negative findings (e.g., \"No evidence of necrosis\"), include them naturally.\n    - Write the explanation as a short medical note (~3â€“5 sentences).\n    - Keep it precise, professional, and easy to understand.\n    \"\"\"\n\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful medical summarizer.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.3\n    )\n    return response.choices[0].message.content.strip()\n\n# --- Main pipeline ---\ndef main():\n    print(\" Starting structured VQA pipeline with OpenAI summarization...\")\n\n    # Load dataset\n    ds = load_dataset(\"SimulaMet/Kvasir-VQA-x1\")[\"test\"]\n    val_set_task2 = (\n        ds.filter(lambda x: x[\"complexity\"] == 1)\n          .shuffle(seed=42)\n          .select(range(1500))  \n          .add_column(\"val_id\", list(range(1500)))\n          .remove_columns([\"complexity\", \"answer\", \"original\"])\n          .cast_column(\"image\", HfImage())\n    )\n    print(f\"Validation set loaded: {len(val_set_task2)} samples\")\n\n    with open(OUTPUT_FILE, \"w\") as f:\n        for item in tqdm(val_set_task2, desc=\"Processing samples\"):\n            val_id = item[\"val_id\"]\n            img_id = item[\"img_id\"]\n            image = item[\"image\"]\n            question_classes = item[\"question_class\"]\n\n            # Step 1: Main Q&A (cleaned)\n            raw_main_answer = ask_model(image, item[\"question\"])\n            cleaned_main_answer = clean_answer(item[\"question\"], raw_main_answer)\n            main_answer = f\"Question: {item['question']}, Answer: {cleaned_main_answer}\"\n\n            # Step 2: Follow-up mapped Q&A (cleaned)\n            mapped_answers = {}\n            for qc in question_classes:\n                mapped_answers[qc] = []\n                for q in mapping.get(qc, []):\n                    raw_ans = ask_model(image, q)\n                    cleaned_ans = clean_answer(q, raw_ans)\n                    mapped_answers[qc].append(f\"Question: {q}, Answer: {cleaned_ans}\")\n\n            # Step 3: Polished textual explanation via OpenAI\n            textual_explanation = summarize_with_openai(main_answer, mapped_answers)\n\n            # Step 4: Save result\n            result = {\n                \"val_id\": val_id,\n                \"img_id\": img_id,\n                \"question\": item[\"question\"],\n                \"answer\": main_answer,\n                # \"mapped_answers\": mapped_answers,\n                \"textual_explanation\": textual_explanation,\n                \"visual_explanation\": [],\n                \"confidence_score\": 0.95\n            }\n            f.write(json.dumps(result) + \"\\n\")\n\n    print(f\"\\n Done! Submission file saved as '{OUTPUT_FILE}'\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -U medvqa\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!medvqa validate_and_submit --competition=medico-2025 --task=2 --repo_id=SujataGaihre/Medico2025_Subtask2_TeamNepal","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}